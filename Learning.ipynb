{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eut0XiJKglMV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import mutual_info_classif #importance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn import svm #Support Vector Machine\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Chest_window30Location=[\"Chest_30Processed\\Process30_chest_S2.csv\",\"Chest_30Processed\\Process30_chest_S3.csv\",\"Chest_30Processed\\Process30_chest_S4.csv\",\"Chest_30Processed\\Process30_chest_S5.csv\",\"Chest_30Processed\\Process30_chest_S6.csv\",\n",
        "                \"Chest_30Processed\\Process30_chest_S7.csv\",\"Chest_30Processed\\Process30_chest_S8.csv\",\"Chest_30Processed\\Process30_chest_S9.csv\",\"Chest_30Processed\\Process30_chest_S10.csv\",\"Chest_30Processed\\Process30_chest_S11.csv\",\n",
        "                \"Chest_30Processed\\Process30_chest_S13.csv\",\"Chest_30Processed\\Process30_chest_S14.csv\",\"Chest_30Processed\\Process30_chest_S15.csv\",\"Chest_30Processed\\Process30_chest_S16.csv\",\"Chest_30Processed\\Process30_chest_S17.csv\"]\n",
        "\n",
        "Chest_window60Location=[\"Chest_60Processed\\Process60_chest_S2.csv\",\"Chest_60Processed\\Process60_chest_S3.csv\",\"Chest_60Processed\\Process60_chest_S4.csv\",\"Chest_60Processed\\Process60_chest_S5.csv\",\"Chest_60Processed\\Process60_chest_S6.csv\",\n",
        "                \"Chest_60Processed\\Process60_chest_S7.csv\",\"Chest_60Processed\\Process60_chest_S8.csv\",\"Chest_60Processed\\Process60_chest_S9.csv\",\"Chest_60Processed\\Process60_chest_S10.csv\",\"Chest_60Processed\\Process60_chest_S11.csv\",\n",
        "                \"Chest_60Processed\\Process60_chest_S13.csv\",\"Chest_60Processed\\Process60_chest_S14.csv\",\"Chest_60Processed\\Process60_chest_S15.csv\",\"Chest_60Processed\\Process60_chest_S16.csv\",\"Chest_60Processed\\Process60_chest_S17.csv\"]\n",
        "\n",
        "Wrist_window30Location=[\"Wrist_30Processed\\Process30_wrist_S2.csv\",\"Wrist_30Processed\\Process30_wrist_S3.csv\",\"Wrist_30Processed\\Process30_wrist_S4.csv\",\"Wrist_30Processed\\Process30_wrist_S5.csv\",\"Wrist_30Processed\\Process30_wrist_S6.csv\",\n",
        "                \"Wrist_30Processed\\Process30_wrist_S7.csv\",\"Wrist_30Processed\\Process30_wrist_S8.csv\",\"Wrist_30Processed\\Process30_wrist_S9.csv\",\"Wrist_30Processed\\Process30_wrist_S10.csv\",\"Wrist_30Processed\\Process30_wrist_S11.csv\",\n",
        "                \"Wrist_30Processed\\Process30_wrist_S13.csv\",\"Wrist_30Processed\\Process30_wrist_S14.csv\",\"Wrist_30Processed\\Process30_wrist_S15.csv\",\"Wrist_30Processed\\Process30_wrist_S16.csv\",\"Wrist_30Processed\\Process30_wrist_S17.csv\"]\n",
        "\n",
        "Wrist_window60Location=[\"Wrist_60Processed\\Process60_wrist_S2.csv\",\"Wrist_60Processed\\Process60_wrist_S3.csv\",\"Wrist_60Processed\\Process60_wrist_S4.csv\",\"Wrist_60Processed\\Process60_wrist_S5.csv\",\"Wrist_60Processed\\Process60_wrist_S6.csv\",\n",
        "                \"Wrist_60Processed\\Process60_wrist_S7.csv\",\"Wrist_60Processed\\Process60_wrist_S8.csv\",\"Wrist_60Processed\\Process60_wrist_S9.csv\",\"Wrist_60Processed\\Process60_wrist_S10.csv\",\"Wrist_60Processed\\Process60_wrist_S11.csv\",\n",
        "                \"Wrist_60Processed\\Process60_wrist_S13.csv\",\"Wrist_60Processed\\Process60_wrist_S14.csv\",\"Wrist_60Processed\\Process60_wrist_S15.csv\",\"Wrist_60Processed\\Process60_wrist_S16.csv\",\"Wrist_60Processed\\Process60_wrist_S17.csv\"]\n",
        "\n",
        "Combine60_Location=[\"Combine_Chest_Wrist\\Combined_S2.csv\",\"Combine_Chest_Wrist\\Combined_S3.csv\",\"Combine_Chest_Wrist\\Combined_S4.csv\",\"Combine_Chest_Wrist\\Combined_S5.csv\",\"Combine_Chest_Wrist\\Combined_S6.csv\",\n",
        "                \"Combine_Chest_Wrist\\Combined_S7.csv\",\"Combine_Chest_Wrist\\Combined_S8.csv\",\"Combine_Chest_Wrist\\Combined_S9.csv\",\"Combine_Chest_Wrist\\Combined_S10.csv\",\"Combine_Chest_Wrist\\Combined_S11.csv\",\n",
        "                \"Combine_Chest_Wrist\\Combined_S13.csv\",\"Combine_Chest_Wrist\\Combined_S14.csv\",\"Combine_Chest_Wrist\\Combined_S15.csv\",\"Combine_Chest_Wrist\\Combined_S16.csv\",\"Combine_Chest_Wrist\\Combined_S17.csv\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVdCm6LyglMX"
      },
      "outputs": [],
      "source": [
        "# Loading function for Random splitting and chest wrist approach\n",
        "\n",
        "def dataset_loading(window,signal_source,chest_wrist):\n",
        "    frame=[]\n",
        "    if window==30 and signal_source==\"Chest\":\n",
        "        StoringLocation=Chest_window30Location\n",
        "    elif window==60 and signal_source==\"Chest\":\n",
        "        StoringLocation=Chest_window60Location\n",
        "    elif window==30 and signal_source==\"Wrist\":\n",
        "        StoringLocation=Wrist_window30Location\n",
        "    elif window==60 and signal_source==\"Wrist\":\n",
        "        StoringLocation=Wrist_window60Location\n",
        "    else:\n",
        "        StoringLocation=Combine60_Location\n",
        "\n",
        "    for filepath in StoringLocation:\n",
        "      file = pd.read_csv(filepath)\n",
        "      frame.append(file)\n",
        "\n",
        "    dataset=pd.concat(frame,ignore_index=True,axis=0)\n",
        "\n",
        "    #remove null value identified from chest signals 30 seocnds csv file\n",
        "    if window==30 and signal_source==\"Chest\":\n",
        "        drop_col=['ecg_hrv_energy_LF', 'ecg_hrv_relative_power_LF', 'ecg_hrv_ratio_LFHF', 'ecg_hrv_norm_LF']\n",
        "        dataset.drop(columns=drop_col,axis=1, inplace=True)\n",
        "        dataset.dropna(inplace=True)\n",
        "\n",
        "    #filtering down to the common signals for chest-wrist approach\n",
        "    if(chest_wrist):\n",
        "        dataset=dataset[[ \"eda_mean\",\"eda_std\",\"eda_min\",\"eda_max\",\"eda_range\",\n",
        "        \"eda_edl_mean\",\"eda_edl_std\",\"eda_edl_slope\",\"eda_edr_std\",\n",
        "        \"eda_edr_num\",\"eda_edr_mean_amp\",\"eda_edr_sum_amp\",\"temp_mean\",\n",
        "        \"temp_std\",\"temp_min\",\"temp_max\",\"temp_range\",\"temp_slope\",\"label\"]]\n",
        "\n",
        "    labels=dataset[\"label\"]\n",
        "\n",
        "    dataset=dataset.drop(\"label\",axis=1)\n",
        "    Unique_class=np.sort(labels.unique())\n",
        "    return dataset, labels,Unique_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05OoSsReY-zH"
      },
      "outputs": [],
      "source": [
        "def standardise_smote(training_x,training_y,valid_x,test_x):\n",
        "    standardiser=StandardScaler()\n",
        "    standardiser.fit(training_x)\n",
        "    training_x=standardiser.transform(training_x)\n",
        "    if(valid_x is not None):\n",
        "      print(\"valiiiii\")\n",
        "      valid_x=standardiser.transform(valid_x)\n",
        "    test_x=standardiser.transform(test_x)\n",
        "\n",
        "    resampling=SMOTE(sampling_strategy=\"auto\")\n",
        "    X_resample_train, Y_resample_train = resampling.fit_resample(training_x, training_y)\n",
        "\n",
        "    return X_resample_train,Y_resample_train,valid_x,test_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZuK6sS2glMX"
      },
      "outputs": [],
      "source": [
        "#conduct random splitting approach + mutual information\n",
        "def dataset_handling(dataset, label,comp_num,sequence):\n",
        "\n",
        "    #mutual information\n",
        "    mutual_info= mutual_info_classif(dataset,label)\n",
        "    mutual_info=pd.Series(mutual_info)\n",
        "    mutual_info.index=dataset.columns.values\n",
        "    mutual_info=mutual_info.sort_values(ascending=False)\n",
        "    # Nair, M. 2023. Feature Selection â€” Mutual Information. Available at:https://medium.com/@miramnair/feature-selection-mutual-information-a0def943e1ed [Accessed:3 May 2025]\n",
        "\n",
        "    X_train,x,Y_train,y=train_test_split(dataset,label, test_size=0.3,random_state=16, stratify=label)\n",
        "    X_valid,X_test,Y_valid,Y_test=train_test_split(x,y,  test_size=0.5, random_state=16,stratify=y)\n",
        "    \n",
        "    X_resample_train,Y_resample_train,X_valid,X_test=standardise_smote(X_train,Y_train,X_valid,X_test)\n",
        "\n",
        "    if (comp_num is not None):\n",
        "        pca_transformer=PCA(n_components=comp_num)\n",
        "        pca_transformer.fit(X_resample_train)\n",
        "        X_resample_train=pca_transformer.transform(X_resample_train)\n",
        "        X_valid=pca_transformer.transform(X_valid)\n",
        "        X_test=pca_transformer.transform(X_test)\n",
        "\n",
        "    Y_valid=Y_valid.reset_index(drop=True)\n",
        "    Y_test=Y_test.reset_index(drop=True)\n",
        "\n",
        "    #Print out to find the class distribution before and after balancing \n",
        "    # print(f\"Y_train is {len(Y_train)}, distribution is {Counter(Y_train)}.\")\n",
        "    # print(f\"After resample(SMOTE) is {len(Y_resample_train)}, distribution is {Counter(Y_resample_train)}.\")\n",
        "    # print(f\"Y_valid is  {len(Y_valid)}, distribution is {Counter(Y_valid)}.\")\n",
        "    # print(f\"Y_test is {len(Y_test)}, distribution is {Counter(Y_test)}\")\n",
        "\n",
        "    return X_resample_train,Y_resample_train,X_valid,Y_valid,X_test,Y_test,mutual_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs6crAjjglMX"
      },
      "outputs": [],
      "source": [
        "# Loading and handling function for subject-wise splitting and average individual approach\n",
        "def individual_Load_handling(window, target_num, comp_num,signal_source, mode):\n",
        "    frame=[]\n",
        "    if window==30 and signal_source==\"Chest\":\n",
        "        StoringLocation=Chest_window30Location\n",
        "    elif window==60 and signal_source==\"Chest\":\n",
        "        StoringLocation=Chest_window60Location\n",
        "    elif window==30 and signal_source==\"Wrist\":\n",
        "        StoringLocation=Wrist_window30Location\n",
        "    elif window==60 and signal_source==\"Wrist\":\n",
        "        StoringLocation=Wrist_window60Location\n",
        "    else:\n",
        "        StoringLocation=Combine60_Location\n",
        "\n",
        "    #randomly select two subject as valid set, another two subject as test set\n",
        "    random.seed(10)\n",
        "    indexpool=random.sample(range(15),k=4)\n",
        "    valid_index=indexpool[:2]\n",
        "    test_index=indexpool[2:]\n",
        "\n",
        "    if(mode==\"average\"):\n",
        "        valid_num=(target_num+1)%15 \n",
        "        for filepath_index in range(len(StoringLocation)):#   ratio of 13:1:1 for each iteration\n",
        "            if(filepath_index==target_num):\n",
        "                file = pd.read_csv(StoringLocation[filepath_index])\n",
        "                test_set=pd.DataFrame(data=file)\n",
        "            elif (filepath_index==valid_num):\n",
        "                file = pd.read_csv(StoringLocation[filepath_index])\n",
        "                valid_set=pd.DataFrame(data=file)\n",
        "            else:\n",
        "                #The rest merge as one\n",
        "                file = pd.read_csv(StoringLocation[filepath_index])\n",
        "                frame.append(file)\n",
        "        dataset=pd.concat(frame,ignore_index=True,axis=0)\n",
        "\n",
        "    else:\n",
        "        valid=[]\n",
        "        test=[]\n",
        "        #ratio of 11:2:2 on approach\n",
        "        for filepath_index in range(len(StoringLocation)):\n",
        "            if (filepath_index in valid_index):\n",
        "                file = pd.read_csv(StoringLocation[filepath_index])\n",
        "                valid.append(file)\n",
        "            elif(filepath_index in test_index ):\n",
        "                file = pd.read_csv(StoringLocation[filepath_index])\n",
        "                test.append(file)\n",
        "            else:\n",
        "                file = pd.read_csv(StoringLocation[filepath_index])\n",
        "                frame.append(file)\n",
        "        dataset=pd.concat(frame,ignore_index=True,axis=0)\n",
        "        valid_set=pd.concat(valid,ignore_index=True,axis=0)\n",
        "        test_set=pd.concat(test,ignore_index=True,axis=0)\n",
        "\n",
        "\n",
        "    if window==30 and signal_source==\"Chest\":#Remove null value found in 30 sec window of chest signals csv\n",
        "        drop_col=['ecg_hrv_energy_LF', 'ecg_hrv_relative_power_LF', 'ecg_hrv_ratio_LFHF', 'ecg_hrv_norm_LF']\n",
        "        dataset.drop(columns=drop_col,axis=1, inplace=True)\n",
        "        dataset.dropna(inplace=True)\n",
        "        valid_set.drop(columns=drop_col,axis=1, inplace=True)\n",
        "        valid_set.dropna(inplace=True)\n",
        "        test_set.drop(columns=drop_col,axis=1, inplace=True)\n",
        "        test_set.dropna(inplace=True)\n",
        "\n",
        "   #Seperate data and label for each\n",
        "    Y_train=dataset[\"label\"]\n",
        "    X_train=dataset.drop(\"label\",axis=1)\n",
        "    Unique_class=np.sort(Y_train.unique())\n",
        "    Y_valid=valid_set[\"label\"]\n",
        "    X_valid=valid_set.drop(\"label\",axis=1)\n",
        "    Y_test=test_set[\"label\"]\n",
        "    X_test=test_set.drop(\"label\",axis=1)\n",
        "\n",
        "    X_resample_train,Y_resample_train,X_valid,X_test=standardise_smote(X_train,Y_train,X_valid,X_test)\n",
        "\n",
        "    #reduce number of features\n",
        "    if (comp_num is not None):\n",
        "        pca_transformer=PCA(n_components=comp_num)\n",
        "        pca_transformer.fit(X_resample_train)\n",
        "        X_resample_train=pca_transformer.transform(X_resample_train)\n",
        "        X_test=pca_transformer.transform(X_test)\n",
        "        X_valid=pca_transformer.transform(X_valid)\n",
        "\n",
        "    return X_resample_train,Y_resample_train,X_valid,Y_valid,X_test,Y_test, Unique_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLL8vCVgglMY"
      },
      "outputs": [],
      "source": [
        "def KNN_classify(training_x,training_y,target_x,k):\n",
        "    kNN=KNeighborsClassifier(n_neighbors=k,weights=\"distance\")\n",
        "    kNN.fit(training_x,training_y)\n",
        "    KNN_classification=kNN.predict(target_x)\n",
        "    return KNN_classification\n",
        "\n",
        "def NaiveBayes_classify(training_x, training_y, target_x):\n",
        "    NBclassifer=GaussianNB()\n",
        "    NBclassifer.fit(training_x,training_y)\n",
        "    GaussianNB_classification=NBclassifer.predict(target_x)\n",
        "    return GaussianNB_classification\n",
        "\n",
        "def RandomForest_classify(training_x,training_y,target_x, tree_number):\n",
        "    RanTree=RandomForestClassifier(n_estimators=tree_number, max_depth=11, min_samples_split=5)\n",
        "    RanTree.fit(training_x,training_y)\n",
        "    RandomTree_classification=RanTree.predict(target_x)\n",
        "    return RandomTree_classification\n",
        "\n",
        "\n",
        "def SVM_classify(training_x, training_y,target_x,C_value):\n",
        "    SVMachine=svm.SVC(kernel='rbf', C=C_value, gamma=\"scale\")\n",
        "    SVMachine.fit(training_x,training_y)\n",
        "    SVM_classification=SVMachine.predict(target_x)\n",
        "    return SVM_classification\n",
        "\n",
        "\n",
        "def MLP_classify(training_x, training_y, target_x, hidden_layer):\n",
        "    MLP_nn = MLPClassifier(hidden_layer_sizes=hidden_layer, activation='relu', solver='adam', max_iter=500)\n",
        "    MLP_nn.fit(training_x,training_y)\n",
        "    MLP_classification=MLP_nn.predict(target_x)\n",
        "    return MLP_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJJWuiZGglMY"
      },
      "outputs": [],
      "source": [
        "def CNN_classify(training_x, training_y, target_x, train_iteration,batch_size):\n",
        "    training_x=tf.reshape(training_x,(training_x.shape[0], training_x.shape[1],1)) \n",
        "    target_x=tf.reshape(target_x,(target_x.shape[0], target_x.shape[1],1))\n",
        "       \n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(training_x.shape[1], training_x.shape[2])))\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.MaxPooling1D(pool_size=2))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(100, activation='relu'))\n",
        "    model.add(layers.Dense(5, activation='softmax'))  # 5-class output\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(training_x, training_y, epochs=train_iteration, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    result = model.predict(target_x)\n",
        "    CNN_classification=np.argmax(result,axis=1)\n",
        "    return CNN_classification\n",
        "    #model implementation as guided by:\n",
        "    #Brownlee, J. 2020. 1D Convolutional Neural Network Models for Human Activity Recognition. Available at: https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/ [Accessed: 27 April 2025]\n",
        "\n",
        "\n",
        "\n",
        "def LSTM_classify(training_x,training_y,target_x,train_iteration,batch_size):\n",
        "    training_x=tf.reshape(training_x,(training_x.shape[0], training_x.shape[1],1)) \n",
        "    target_x=tf.reshape(target_x,(target_x.shape[0], target_x.shape[1],1))\n",
        "   \n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.LSTM(units=32, activation='relu',input_shape=(training_x.shape[1], training_x.shape[2])))  \n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    model.fit(training_x, training_y, epochs=train_iteration, batch_size=batch_size, verbose=0)\n",
        "    result = model.predict(target_x)\n",
        "    LSTM_classification=np.argmax(result,axis=1)\n",
        "    return LSTM_classification\n",
        "    #model implementation as guided by:\n",
        "    #n1k31t4, 2018. Keras LSTM with 1D time series. Data Science Stack Exchange. Available at: https://datascience.stackexchange.com/questions/27533/keras-lstm-with-1d-time-series [Accessed: 27 April 2025]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJOo5XnlglMZ"
      },
      "outputs": [],
      "source": [
        "#functions for finding the score from evaluation metrics function\n",
        "def evaluate_performance(truth, predict,class_list, individually_bool):\n",
        "    Performance_class={}\n",
        "    predict=pd.Series(predict)\n",
        "\n",
        "    acc=accuracy_score(y_true=truth,y_pred=predict)\n",
        "    precision=precision_score(y_true=truth,y_pred=predict,average=\"weighted\")\n",
        "    recall=recall_score(y_true=truth,y_pred=predict,average=\"weighted\",zero_division=0)\n",
        "    f1Score=f1_score(y_true=truth, y_pred=predict,average=\"weighted\",zero_division=0)\n",
        "    recall_array=recall_score(y_true=truth,y_pred=predict,average=None,labels=class_list,zero_division=0)\n",
        "    f1Score_array=f1_score(y_true=truth, y_pred=predict,average=None,labels=class_list,zero_division=0)\n",
        "    if(individually_bool==True):\n",
        "        overall_result=[acc,precision,recall,f1Score]\n",
        "        for label in class_list:\n",
        "            overall_result.append(recall_array[label])\n",
        "            overall_result.append(f1Score_array[label])\n",
        "\n",
        "        overall_result = pd.DataFrame([overall_result],\n",
        "                                      columns=[\"accuracy_score\",\"precision\",\"recall\",\"f1_score\",\n",
        "                                               \"class_0_recall\",\"class_0_f1_score\",\n",
        "                                               \"class_1_recall\",\"class_1_f1_score\",\n",
        "                                               \"class_2_recall\",\"class_2_f1_score\",\n",
        "                                               \"class_3_recall\",\"class_3_f1_score\",\n",
        "                                               \"class_4_recall\",\"class_4_f1_score\"])\n",
        "\n",
        "    else:\n",
        "        overall_result={\"accuracy\":acc, \"precision\":precision,\"recall\":recall,\"f1_score\":f1Score}\n",
        "        for label in class_list:\n",
        "            Performance_class[label]={ \"class_recall\":recall_array[label],\"class_f1_score\":f1Score_array[label]}\n",
        "        overall_result[\"individual\"]=Performance_class\n",
        "    return overall_result\n",
        "\n",
        "# Result print out in text format\n",
        "def final_printout(final_result):\n",
        "    for window, info in final_result.items():\n",
        "        print(f\"from {window} seconds window:\")\n",
        "        print(\"-----------\")\n",
        "        for method, result in info.items():\n",
        "            print(f\"With {method} method the result is: \")\n",
        "            for metric, data in result.items():\n",
        "                if metric ==\"individual\":\n",
        "                    for individual, ind_result in data.items():\n",
        "                        print(f\"For {individual} class: {ind_result}\")\n",
        "                else:\n",
        "                    print(f\"{metric} is: {data}\")\n",
        "            print(\"++++++++++++++++++\")\n",
        "        print(\"==================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxpBWqEXglMZ"
      },
      "outputs": [],
      "source": [
        "#function to execute the classifiers and collect the classification result\n",
        "def classifier_collection(X_resample_train,Y_resample_train,X_target,Y_target,Signal_parameter,signal_source,class_list,hidden_layer,individually):\n",
        "    Performance_stats={\"KNN\":{}, \"NB\":{}, \"RF\":{}, \"SVM\":{},\"MLP\":{},\"CNN\":{},\"LSTM\":{}}\n",
        "    Performance_stats[\"KNN\"].update(evaluate_performance(Y_target,KNN_classify(training_x=X_resample_train,training_y=Y_resample_train,target_x=X_target,k=Signal_parameter[signal_source][\"k_number\"]),class_list=class_list,individually_bool=individually))\n",
        "    Performance_stats[\"NB\"].update(evaluate_performance(Y_target,NaiveBayes_classify(training_x=X_resample_train,training_y=Y_resample_train,target_x=X_target),class_list=class_list,individually_bool=individually))\n",
        "    Performance_stats[\"RF\"].update(evaluate_performance(Y_target,RandomForest_classify(training_x=X_resample_train,training_y=Y_resample_train,target_x=X_target,tree_number=Signal_parameter[signal_source][\"Tree_number\"]),class_list=class_list,individually_bool=individually))\n",
        "    Performance_stats[\"SVM\"].update(evaluate_performance(Y_target,SVM_classify(training_x=X_resample_train,training_y=Y_resample_train,target_x=X_target,C_value=Signal_parameter[signal_source][\"C_value\"]),class_list=class_list,individually_bool=individually))\n",
        "    Performance_stats[\"MLP\"].update(evaluate_performance(Y_target,MLP_classify(training_x=X_resample_train,training_y=Y_resample_train,target_x=X_target,hidden_layer=hidden_layer),class_list=class_list,individually_bool=individually))\n",
        "    Performance_stats[\"CNN\"].update(evaluate_performance(Y_target,CNN_classify(training_x=X_resample_train,training_y=Y_resample_train,target_x=X_target,train_iteration=Signal_parameter[signal_source][\"CNN_train_itera\"], batch_size=Signal_parameter[signal_source][\"CNN_batch_size\"]),class_list=class_list,individually_bool=individually))\n",
        "    Performance_stats[\"LSTM\"].update(evaluate_performance(Y_target,LSTM_classify(training_x=X_resample_train,training_y=Y_resample_train,target_x=X_target,train_iteration=Signal_parameter[signal_source][\"LSTM_train_itera\"], batch_size=Signal_parameter[signal_source][\"LSTM_batch_size\"]),class_list=class_list,individually_bool=individually))\n",
        "    return Performance_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkrFA-_aglMZ"
      },
      "outputs": [],
      "source": [
        "#Random Splitting Approach\n",
        "def random_split_process(windows, signal_source, Signal_parameter,Testset_stage,individually):\n",
        "    featureimportance_list=[]\n",
        "    Final_Result={}\n",
        "    for window in windows:\n",
        "        print(window)\n",
        "\n",
        "        #load and handle the data\n",
        "        dataset,label,class_list=dataset_loading(window,signal_source,False)\n",
        "        X_resample_train,Y_resample_train,X_valid,Y_valid,X_test,Y_test, importance_list= dataset_handling(dataset,label,Signal_parameter[signal_source][\"PCA_target\"],window)\n",
        "        hidden_layer=Signal_parameter[signal_source][\"hidden_layer\"]\n",
        "        \n",
        "        featureimportance_list.append(importance_list)\n",
        "        \n",
        "        if Testset_stage==False:\n",
        "            Final_Result[window]=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                    X_target=X_valid,Y_target=Y_valid,Signal_parameter=Signal_parameter,\n",
        "                                                    signal_source=signal_source,class_list=class_list,\n",
        "                                                    hidden_layer=hidden_layer,individually=individually)\n",
        "        else:\n",
        "            Final_Result[window]=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                   X_target=X_test,Y_target=Y_test,Signal_parameter=Signal_parameter,\n",
        "                                                   signal_source=signal_source,class_list=class_list,\n",
        "                                                   hidden_layer=hidden_layer,individually=individually)\n",
        "\n",
        "    return Final_Result,featureimportance_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWXfEiNQglMa"
      },
      "outputs": [],
      "source": [
        "#Subject-wise Splitting Approach\n",
        "def Subject_split_learning(windows, Signal_parameter, signal_source, test_stage):\n",
        "    Final_Result={}\n",
        "    for window in windows:\n",
        "        print(window)\n",
        "        #pre-handling \n",
        "        X_resample_train,Y_resample_train,X_valid,Y_valid,X_test,Y_test, Unique_class=individual_Load_handling(window=window,target_num=None,comp_num=Signal_parameter[signal_source][\"PCA_target\"],signal_source=signal_source,mode=\"subject\")\n",
        "        hidden_layer=Signal_parameter[signal_source][\"hidden_layer\"]\n",
        "\n",
        "        if (test_stage):\n",
        "          Final_Result[window]=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                   X_target=X_test,Y_target=Y_test,Signal_parameter=Signal_parameter,\n",
        "                                                   signal_source=signal_source,class_list=Unique_class,\n",
        "                                                   hidden_layer=hidden_layer,individually=False)\n",
        "        else:\n",
        "          Final_Result[window]=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                   X_target=X_valid,Y_target=Y_valid,Signal_parameter=Signal_parameter,\n",
        "                                                   signal_source=signal_source,class_list=Unique_class,\n",
        "                                                   hidden_layer=hidden_layer,individually=False)\n",
        "\n",
        "    return Final_Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vedvRl6uglMa"
      },
      "outputs": [],
      "source": [
        "#Average Individual Approach\n",
        "def individual_learning_process(windows, Signal_parameter,signal_source, individually, test_stage):\n",
        "    Individual_result=pd.DataFrame()\n",
        "    for window in windows:\n",
        "        print(window)\n",
        "        Individual_window={\"KNN\":[],\"NB\":[],\"RF\":[],\"SVM\":[],\"MLP\":[],\"CNN\":[],\"LSTM\":[]}\n",
        "        for target_index in range(15):   #loop through every subject\n",
        "            print(f\"Processing {target_index}\")\n",
        "            #Load in and pre handle the data for learning\n",
        "            X_resample_train,Y_resample_train,X_valid,Y_valid,X_test,Y_test, class_list=individual_Load_handling(window=window,target_num=target_index,comp_num=Signal_parameter[signal_source][\"PCA_target\"],signal_source=signal_source,mode=\"average\")\n",
        "            hidden_layer=Signal_parameter[signal_source][\"hidden_layer\"]\n",
        "            \n",
        "            if(test_stage):\n",
        "              result=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                    X_target=X_test,Y_target=Y_test,Signal_parameter=Signal_parameter,\n",
        "                                                    signal_source=signal_source,class_list=class_list,\n",
        "                                                    hidden_layer=hidden_layer,individually=individually)\n",
        "            else:\n",
        "              result=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                    X_target=X_valid,Y_target=Y_valid,Signal_parameter=Signal_parameter,\n",
        "                                                    signal_source=signal_source,class_list=class_list,\n",
        "                                                    hidden_layer=hidden_layer,individually=individually)\n",
        "            \n",
        "            for key, value in result.items():\n",
        "              if key in Individual_window:\n",
        "                df=pd.DataFrame.from_dict(result[key]).T\n",
        "                Individual_window[key].append(df)\n",
        "\n",
        "\n",
        "        #merge and process the average result from every classifier\n",
        "        for key in Individual_window.keys():\n",
        "            Individual_window[key]=pd.concat(Individual_window[key],ignore_index=True,axis=1)\n",
        "            Indi_mean=Individual_window[key].mean(axis=1)\n",
        "            Indi_mean=Indi_mean.to_frame()\n",
        "            Indi__classifier_mean=Indi_mean.rename(columns={0:f\"{window}-{key}\"})\n",
        "            Individual_result=pd.concat([Individual_result,Indi__classifier_mean],axis=1)\n",
        "\n",
        "    return Individual_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAykYNhWglMa"
      },
      "outputs": [],
      "source": [
        "#Chest-Wrist Approach\n",
        "def Chest_Wrist_learning(windows, Signal_parameter,individually, direction,test_stage):\n",
        "    Final_Result={}\n",
        "    signal_sources=list(Signal_parameter.keys())\n",
        "    if direction==0:\n",
        "        train_source,test_source=signal_sources[0],signal_sources[1]\n",
        "        print(\"Trained by Wrist signal, Classify Chest Signal\")\n",
        "    else:\n",
        "        train_source,test_source=signal_sources[1],signal_sources[0]\n",
        "        print(\"Trained by Chest signal, Classify Wrist Signal\")\n",
        "\n",
        "    for window in windows:\n",
        "        print(window)\n",
        "        X_train,Y_train,train_unique=dataset_loading(window,train_source, True)\n",
        "        X_target,Y_target,test_unique=dataset_loading(window,test_source, True)\n",
        "\n",
        "        X_valid,X_test,Y_valid,Y_test=train_test_split(X_target,Y_target,  test_size=0.5, random_state=16,stratify=Y_target)\n",
        "\n",
        "        X_resample_train,Y_resample_train,X_valid,X_test=standardise_smote(X_train,Y_train,X_valid,X_test)\n",
        "        hidden_layer=Signal_parameter[train_source][\"hidden_layer\"]\n",
        "        \n",
        "        if(test_stage):\n",
        "            Final_Result[window]=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                    X_target=X_test,Y_target=Y_test,Signal_parameter=Signal_parameter,\n",
        "                                                    signal_source=train_source,class_list=train_unique,\n",
        "                                                    hidden_layer=hidden_layer,individually=individually)\n",
        "\n",
        "        else:\n",
        "            Final_Result[window]=classifier_collection(X_resample_train=X_resample_train,Y_resample_train=Y_resample_train,\n",
        "                                                    X_target=X_valid,Y_target=Y_valid,Signal_parameter=Signal_parameter,\n",
        "                                                    signal_source=train_source,class_list=train_unique,\n",
        "                                                    hidden_layer=hidden_layer,individually=individually)\n",
        "    return Final_Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtWtGFdQglMZ"
      },
      "outputs": [],
      "source": [
        "#find average and top ranked feature\n",
        "def feature_ranking(featureimportance_list):\n",
        "    frame=pd.concat(featureimportance_list,axis=1)\n",
        "    frame=frame.fillna(0)\n",
        "    f_mean=frame.mean(axis=1)\n",
        "    frame[\"Mean\"]= f_mean\n",
        "    final=frame.sort_values(by=\"Mean\", ascending=False)\n",
        "    final=final.rename(columns={0:\"30 Second window\",1:\"60 Second window\"})\n",
        "    print(\"Combined importance of features from both windows\")\n",
        "    display(final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdJBGnBq0Qf3"
      },
      "outputs": [],
      "source": [
        "#Paremeter management for each approach and source\n",
        "def parameter_control(subject_bool, average_bool, chest_wrist_bool):\n",
        "  if(subject_bool):\n",
        "    Signal_parameter={\n",
        "    \"Wrist\":{\"PCA_target\":18,\"k_number\":31,\"Tree_number\":100,\"C_value\":1,\"hidden_layer\":(54,54,54),\"CNN_train_itera\":10,\"CNN_batch_size\":32,\"LSTM_train_itera\":15,\"LSTM_batch_size\":48},#\n",
        "    \"Chest\":{\"PCA_target\":0.95,\"k_number\":31,\"Tree_number\":200,\"C_value\":1,\"hidden_layer\":(52,52,52),\"CNN_train_itera\":10,\"CNN_batch_size\":48, \"LSTM_train_itera\":10,\"LSTM_batch_size\":48},#\n",
        "    \"Combined\":{\"PCA_target\":0.95,\"k_number\":31,\"Tree_number\":200,\"C_value\":1,\"hidden_layer\":(84,84,84),\"CNN_train_itera\":10,\"CNN_batch_size\":32,\"LSTM_train_itera\":15,\"LSTM_batch_size\":64}#\n",
        "      }\n",
        "  elif(average_bool):\n",
        "    Signal_parameter={\n",
        "      \"Wrist\":{\"PCA_target\":18,\"k_number\":21,\"Tree_number\":100,\"C_value\":1,\"hidden_layer\":(90,90,90),\"CNN_train_itera\":10,\"CNN_batch_size\":48,\"LSTM_train_itera\":15,\"LSTM_batch_size\":48},#\n",
        "      \"Chest\":{\"PCA_target\":0.95,\"k_number\":21,\"Tree_number\":100,\"C_value\":1,\"hidden_layer\":(52,52,52),\"CNN_train_itera\":10,\"CNN_batch_size\":48, \"LSTM_train_itera\":15,\"LSTM_batch_size\":48},#\n",
        "      \"Combined\":{\"PCA_target\":0.95,\"k_number\":15,\"Tree_number\":150,\"C_value\":1,\"hidden_layer\":(84,84,84),\"CNN_train_itera\":10,\"CNN_batch_size\":32,\"LSTM_train_itera\":15,\"LSTM_batch_size\":48}#\n",
        "      }\n",
        "  elif(chest_wrist_bool):\n",
        "      Signal_parameter={\n",
        "      \"Wrist\":{\"PCA_target\":None,\"k_number\":21,\"Tree_number\":150,\"C_value\":5,\"hidden_layer\":(54,54,54),\"CNN_train_itera\":10,\"CNN_batch_size\":32,\"LSTM_train_itera\":10,\"LSTM_batch_size\":32},\n",
        "      \"Chest\":{\"PCA_target\":None,\"k_number\":21,\"Tree_number\":150,\"C_value\":1,\"hidden_layer\":(54,54,54),\"CNN_train_itera\":10,\"CNN_batch_size\":32, \"LSTM_train_itera\":10,\"LSTM_batch_size\":32}\n",
        "      }\n",
        "  else:#random\n",
        "     Signal_parameter={\n",
        "      \"Wrist\":{\"PCA_target\":18,\"k_number\":71,\"Tree_number\":100,\"C_value\":1,\"hidden_layer\":(54,54,54),\"CNN_train_itera\":10,\"CNN_batch_size\":32,\"LSTM_train_itera\":18,\"LSTM_batch_size\":32},\n",
        "      \"Chest\":{\"PCA_target\":0.95,\"k_number\":71,\"Tree_number\":100,\"C_value\":1,\"hidden_layer\":(52,52,52),\"CNN_train_itera\":10,\"CNN_batch_size\":32, \"LSTM_train_itera\":15,\"LSTM_batch_size\":32},\n",
        "      \"Combined\":{\"PCA_target\":0.95,\"k_number\":71,\"Tree_number\":100,\"C_value\":1,\"hidden_layer\":(84,84,84),\"CNN_train_itera\":10,\"CNN_batch_size\":32,\"LSTM_train_itera\":15,\"LSTM_batch_size\":32}\n",
        "      }\n",
        "  return Signal_parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "cmjIB24FI3rZ",
        "outputId": "62ae74b6-e055-4076-ec79-7454be3e9a2f"
      },
      "outputs": [],
      "source": [
        "#main flow: call the classify function and evaluate function\n",
        "\n",
        "signal_source=\"Chest\" #\"Combined\"#\"Wrist\"\n",
        "if signal_source ==\"Combined\":\n",
        "  windows=[60]\n",
        "else:\n",
        "  windows=[30,60] # 30 or 60\n",
        "\n",
        "\n",
        "#Boolean to control the apporach for execution\n",
        "Testset_stage=False\n",
        "subject_split=False\n",
        "average_individually=False\n",
        "Chest_Wrist_Mix=False\n",
        "Chest_wrist_direction=0 #0= Wrist train -> Chest test   #1=Chest train -> Wrist test\n",
        "\n",
        "#select corresponding parameter\n",
        "Signal_parameter=parameter_control(subject_split,average_individually,Chest_Wrist_Mix)\n",
        "\n",
        "#Approach\n",
        "if(average_individually):#one out at a time and average it\n",
        "    Individual_result=individual_learning_process(windows,Signal_parameter,signal_source,average_individually,Testset_stage)\n",
        "elif(subject_split):#subject-wise splitting\n",
        "    Final_Result=Subject_split_learning(windows, Signal_parameter,signal_source,Testset_stage)\n",
        "elif(Chest_Wrist_Mix):#Chest or Wrist classify the other\n",
        "    Final_Result=Chest_Wrist_learning(windows, Signal_parameter,False,Chest_wrist_direction,Testset_stage)\n",
        "else:#random split from the merged\n",
        "    Final_Result,featureimportance_list=random_split_process(windows, signal_source, Signal_parameter,Testset_stage,individually=False)\n",
        "\n",
        "\n",
        "#result\n",
        "print(f\"signal source: {signal_source}\")\n",
        "if (average_individually):\n",
        "    print(\"Average individually\")\n",
        "    #directly displaying the dataframe\n",
        "    display(Individual_result)\n",
        "else:\n",
        "    #result presentation\n",
        "    if( Testset_stage==True):\n",
        "        print(\"Test Set\")\n",
        "    else:\n",
        "        print(\"Valid Set\")\n",
        "    if(Chest_Wrist_Mix):\n",
        "        print(\"Chest Wrist set\")\n",
        "    elif(subject_split):\n",
        "        print(\"Split by num of Subject\")\n",
        "    else:\n",
        "        print(\"Random Split\")\n",
        "    final_printout(Final_Result)\n",
        "    #feature_ranking(featureimportance_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
